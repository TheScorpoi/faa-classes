{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lab: Multivariable Linear Regression\n",
    "Objectives: Implement linear regression with multiple variables (features) and get to see it works on data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file **Multi_linear.txt** contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.  \n",
    "\n",
    "Load the data into the variable data (using function pd.read_csv from panda library). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from the dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Create scatter plots of data similar to Fig.1 (using plt.scatter)\n",
    "\n",
    "<img src=\"images/f5.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> Fig.1: Data visualtization </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(?,?)\n",
    "......\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(?,?....)\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Note that house sizes are much larger values (about 1000 times) than the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. \n",
    "To make sure features are on a similar scale apply Mean normalization.\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "Your task is to complete the code in function **featureNormalization(X)**:\n",
    "\n",
    "• Compute the mean value  $\\mu_i$ of each feature (use np.mean(X,axis=0)) \n",
    "\n",
    "• compute the standard deviation $\\sigma_i$ of each feature (use np.std(X,axis=0)) \n",
    "\n",
    "• Apply the equation above.\n",
    "\n",
    "**REMARK !!!:** When normalizing the features, it is important to store the mean value and the standard deviation used for normalization. After optimizing the parameters of the model, you want to predict the price of a new example not seen before.\n",
    "You must first normalize the features of that new example using the mean and standard deviation previously computed from the training set.\n",
    "\n",
    "**Remark:** Mean normalization is an alternative to normalizing by making the absolute values < 1 (i.e. dividing by MaxValue-MinValue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mean=?\n",
    "    std=?\n",
    "    \n",
    "    X_norm = ?\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function $J(\\theta)$\n",
    "\n",
    "The objective of Linear Regression is to minimize the cost function: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)} )^2$\n",
    "\n",
    "where *h* is the linear model: $h_\\theta(x)=\\theta^Tx=\\theta_0+\\theta_1x_1+....+\\theta_nx_n$ \n",
    "\n",
    "Complete function **computeCost(X,y,theta)**.  Remember that the variables X and y are not scalar values, X is an array (matrix) with dimension (*mx3*), y is an array (vector) with dimension (*mx1*), *m* rows represent the examples from the training set.\n",
    "\n",
    "Suggestion: Use the vectorized dot product with *np.dot()* to generate *h*. \n",
    "Use *np.sum()* to compute the sum of errors over all given examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    \"\"\"\n",
    "   Take the numpy arrays X, y, theta and return the cost function J for this theta. \n",
    "\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m=\n",
    "    \n",
    "    #linear regression model\n",
    "    h= ?\n",
    "    \n",
    "    #cost function\n",
    "    J= ?\n",
    "\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will provide values for the arguments of **computeCost(X,y,theta)**. \n",
    "First, extract X and y from data. \n",
    "\n",
    "**Check if y is rank 1 arrays (m,)**  and if yes, you need to reshape them to be 2-dimensonal arrays (m,1).  \n",
    "Each example is stored as a row.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n=data.values # extract only the values of the two columns from the dataFrame data\n",
    "X= ?\n",
    "y= ?\n",
    "\n",
    "\n",
    "#Run featureNormalization to normalize X, store the means and stds.\n",
    "X, mean_X, std_X = ?\n",
    "\n",
    "\n",
    "#To take into account the intercept term theta_0 you need to add an additional first column to X and \n",
    "#set it to all ones (np.ones). #This allows to treat theta_0 as simply another ‘feature parameter’. \n",
    "\n",
    "X=np.append(?,?) \n",
    "\n",
    "#Initialize the fitting parameters theta to 0 (np.zeros)\n",
    "\n",
    "theta_init=\n",
    "\n",
    "#You should see a cost of about  65591548106.46\n",
    "\n",
    "computeCost(X,y,theta_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Minimize the cost function $J(\\theta)$ by updating Equation        \n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\\theta_j$ for all $j$) and repeat unitil convergence. \n",
    "\n",
    "\n",
    "Implement gradient descent in the function **gradientDescent**. The loop structure is written, you need to supply the updates to $\\theta$  within each iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take numpy arrays X, y and theta and update theta by taking num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "     Return: theta and the list of the cost of theta (J_history) during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples\n",
    "    m= ?\n",
    "    J_history=[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # model\n",
    "        h = ?\n",
    "        \n",
    "        #Vectorized way to compute all gradients simultaneously \n",
    "        grad = np.dot(X.transpose(),(h-y)) \n",
    "        \n",
    "        #update \n",
    "        theta= ?\n",
    "        \n",
    "        J_history.append(computeCost(X,y,theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradientDescent with learning rate 0.1 and 400 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, J_history = gradientDescent(?,?,?,?,?)\n",
    "print(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1 + \"+str(round(theta[2,0],2))+\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the implementation\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to plot $J(\\theta)$ against the number of iteration. Function **gradientDescent** calls function **computeCost** on every iteration and saves the costs over the iterations. If the algorithm works properly, $J(\\theta)$ should never increase, and should converge to a steady value. \n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 2 Cost function for different learning rates </center></caption>\n",
    "\n",
    "If the learning rate is too small (e.g. 0.001), the gradient descent takes a very long time to converge to the optimal value. \n",
    "\n",
    "If the learning rate is too large (e.g. 1.4), $J(\\theta)$ can diverge and \"blow up\", resulting in values which are too large for computer calculations. In these situations, Python will return nan (not a number). This is often caused by undefined operations that involve +/- infinity.\n",
    "\n",
    "Apply **gradientDescent** with different learning rates (e.g. alpha=[0.001, 0.01, 0.1, 0.3 1.4] ) and 400 iterations.\n",
    "\n",
    "Plot the gradient history and get a curve similar to Fig.2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha= ?\n",
    "\n",
    "for i in alpha:\n",
    "    theta_init=np.zeros((n+1,1))\n",
    "    theta, J_history = gradientDescent(?,?,?,?,?)\n",
    "     plt.plot(?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions  using the optimized $\\theta$ values\n",
    "\n",
    "Complete function **predict** to compute model predictions: $h_\\theta(x) = \\theta^Tx$. Apply vectorized computations with np.dot(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,theta):\n",
    "    \"\"\"\n",
    "    Takes in numpy array of x and theta and return the predicted value of y based on theta\n",
    "    \"\"\"\n",
    "    \n",
    "    # model\n",
    "    h = ?\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best learning rate you found, run gradient descent until convergence to find the optimal $\\theta$ values.\n",
    "\n",
    "Predict the price of a house with 1650 square feet and 3 bedrooms (use function predict you have implemented in Part 1). \n",
    "\n",
    "Don't forget to normalize the features, before making this prediction!\n",
    "\n",
    "Answer: the price is about $293080.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample =?\n",
    "#feature normalisation of x_sample\n",
    "x_sample= ?\n",
    "#add 1\n",
    "x_sample=\n",
    "\n",
    "predict3=predict(?,?)\n",
    "print(\"For house size = 1650, # of bedroom = 3, we predict a house value of $\"+str(round(predict3,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
