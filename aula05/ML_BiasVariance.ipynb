{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab- Linear Regression with L2 Regularization - Bias-Variance tradeoff\n",
    "\n",
    "**Objectives**: Implement Regularized Linear Regression algorithm and use it to study the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The task is to implement regularized linear regression to predict the amount of water owing out of a dam using the change of water level in a reservoir. \n",
    "\n",
    "You will examine the effects of bias versus variance.\n",
    "\n",
    "File *ex5data1.mat* contains historical records on the change in the water level, x, and the amount of water owing out of the dam, y. The dataset is divided into the following parts:\n",
    "\n",
    "• Training set ( X, y) used to fit the model.\n",
    "\n",
    "• Cross Validation (CV) / dev set (Xval, yval) for determining the regularization parameter.\n",
    "\n",
    "• Test set (Xtest, ytest) for evaluating performance. These are examples which the model did not see during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loadmat to load the matlab file ex5data1.mat and extract train, val and test subsets. \n",
    "\n",
    "X= ?\n",
    "y= ?\n",
    "\n",
    "Xval=?\n",
    "yval=?\n",
    "\n",
    "Xtest= ?\n",
    "ytest= ?\n",
    "\n",
    "m = ?  # Number of training examples \n",
    "mval = ? # Number of CV/dev examples \n",
    "mtest = ?  # Number of testing examples \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Plot the training data and get a figure similar to Fig.1. \n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **Training data** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fig.1. Consult similar code from previous labs \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression Cost (Loss) Function\n",
    "\n",
    "Now, you will implement Linear Regression to fit a straight line to the data and plot the learning curves. \n",
    "\n",
    "The regularized Linear Regression Cost (Loss) Function is:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\theta_j^2)$\n",
    "\n",
    "Recall that $\\lambda$ is the regularization parameter which helps preventing overfitting. The regularization term puts a penalty on the overall cost $J(\\theta)$, $\\theta_0$  is not regularized.  \n",
    "\n",
    "Complete the code in function *linearRegCostFunction* to calculate the Regularized Linear Regression Cost (Loss) function and its gradients with respect to  $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, Lambda):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take in numpy array of  data X, labels y and theta, to return the regularized cost function and gradients\n",
    "    of the linear regression model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of training examples \n",
    "    m = ?\n",
    "    \n",
    "    #linear regression model\n",
    "    h = ?\n",
    "    \n",
    "    cost = 1/(2*m) * np.sum((h - y)**2)\n",
    "    reg_cost = cost + Lambda/(2*m) * (np.sum(theta[1:]**2))\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad_0= (1/m) * np.dot(X.transpose(),(h - y))[0]\n",
    "    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:] + (Lambda/m)* theta[1:]\n",
    "       \n",
    "    #  make the complete gradient a column vector\n",
    "    grad_all=np.append(grad_0,grad)\n",
    "    grad_all = grad_all.reshape((len(grad_all), 1))\n",
    "    \n",
    "    return reg_cost, grad_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Linear Regression\n",
    "\n",
    "Once the cost function and the gradients are computed correctly, run *gradientDescent* to compute the optimal values of $\\theta$. \n",
    "\n",
    "Here, we set the regularization parameter $\\lambda$ = 0. Because the linear regression is trying to fit a 2D $\\theta$, regularization will not be much helpful for $\\theta$ of such low dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,learn_rate,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = linearRegCostFunction(X,y,theta,Lambda)\n",
    "        theta = theta - (learn_rate * grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra columns of 1 to X (recall axis=0 are the rows, axis=1 are the columns)\n",
    "X_1 = \n",
    "\n",
    "Lambda = 0\n",
    "learn_rate=0.001\n",
    "#choose an adequate number of iterations such that the cost function converged \n",
    "num_iter= ?\n",
    "\n",
    "# inicialize theta vector with 0. \n",
    "initial_theta = ?\n",
    "\n",
    "#compute the optimal theta : EXPECTED RESULT: theta= [~12; ~0.36]\n",
    "theta, J_history = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Function history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(?)\n",
    "#add labels\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and its linear model\n",
    "You should get a figure similar to Fig.2. This best fit line tells that the model is not a good fit to the data. \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.2** : **Linear fit** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add the scatter plot of data (as above) \n",
    "?\n",
    "\n",
    "\n",
    "#Plot the best linear model\n",
    "x_fit=range(-50,40)\n",
    "y_fit=theta[0]+theta[1]*x_fit\n",
    "plt.plot(x_fit,y_fit,color=\"b\")\n",
    "\n",
    "plt.ylim(-5,40)\n",
    "plt.xlim(-50,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance and learning curves \n",
    "\n",
    "An important concept in ML is the **bias-variance** tradeoff. \n",
    "\n",
    "Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data. \n",
    "\n",
    "Now, you will plot training and CV/dev learning curves to diagnose bias-variance problems. \n",
    "To plot the learning curve, we need training and CV errors for different training set sizes. To obtain different training set sizes, *learningCurve* use different subsets of the original training set X. Specifically, for a training set size of i, the first i examples (i.e., X(0:i,:) and y(0:i)) are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, learn_rate, num_iter, Lambda):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the train and cross validation set errors for a learning curve\n",
    "    \"\"\"\n",
    "   \n",
    "    m = ?  # Number of training examples \n",
    "    n=?  # number of features \n",
    "    mval = ?  # Number of validation examples \n",
    "\n",
    "    error_train, error_val = [],[]\n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        # inicialize theta vector with 0. \n",
    "        initial_theta= ?\n",
    "        Xtrain=X[0:i,:]\n",
    "        ytrain=y[0:i,:]\n",
    "        #compute the optimal theta :\n",
    "        theta = gradientDescent( Xtrain, ytrain,initial_theta,learn_rate,num_iter,Lambda)[0]\n",
    "        \n",
    "        h_train = np.dot(Xtrain, theta)\n",
    "        h_val = np.dot(Xval, theta)\n",
    "        \n",
    "        error_train_i = 1/(2*m) * np.sum((h_train - ytrain)**2)\n",
    "        error_val_i = 1/(2*mval) * np.sum((h_val - yval)**2)\n",
    "        \n",
    "        error_train.append(error_train_i)\n",
    "        error_val.append(error_val_i)\n",
    "\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra columns of 1 to Xval\n",
    "Xval_1 = ?\n",
    "\n",
    "#Call learningCurve to compute E_train and E_validation\n",
    "error_train, error_val = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curves\n",
    "\n",
    "Plot the learning curves as shown in Fig. 3. You can observe that both the training and the cross validation errors are high even when the number of training examples increases. This reflects a high bias problem of the model. \n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.3** : **Linear Regression learning curves** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,m+1),error_train)\n",
    "plt.plot(range(1,m+1), error_val,color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend([\"Train\", \"Cross Validation\"])\n",
    "plt.ylim(-10,210)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "The linear model is too simple for this data and resulted in underfitting (high bias). Now, you will address this problem by adding more features using higher powers of the original feature (waterLevel), such as:\n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:450px;height:50px;\">\n",
    "<caption><center> **Fig.4**: **Polynomial Regression model** </center></caption>\n",
    "\n",
    "Keep in mind that even though we have polynomial terms in the model above, we are still solving a linear regression optimization problem. The polynomial terms are simply new features that we can use for linear regression. The function *polyFeatures* maps the original training set X of size mx1 into its higher powers. Specifically, when a training set X of size mx1 is passed into the function, the function should return a mxp matrix X_poly, where 1st column holds the original values of X, 2nd column holds the values of $X^2$,  3rd column holds the values of $X.^3$, and so on. \n",
    "\n",
    "Function *polyFeatures* is applied to the training, cross validation and test data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, degree):\n",
    "    \"\"\"\n",
    "    Takes a data matrix X (size m x 1) and maps each example into its polynomial features where \n",
    "    X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^degree];\n",
    "    \"\"\"\n",
    "    m= ?  # Number of training examples \n",
    "    \n",
    "    for i in range(2,degree+1):\n",
    "        var=X[:,0]**i\n",
    "        X = np.append(X, var.reshape(m,1), axis=1)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "After adding polynomial terms, the new features are badly scaled. Let assume a polynomial of degree 8, an example with x = 40 will now have a feature x8 = 40^8 = 6.5x10^12. Therefore, before training the model, the features of the training set are first normalized. Here *StandardScaler()* function is used, that standardize the features by removing the mean and scaling to unit variance. Check *StandardScaler?* for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Map X onto Polynomial features (call polyFeatures)\n",
    "degree=8\n",
    "X_poly = ?\n",
    "\n",
    "#Check the range of values for the polynomial features\n",
    "?\n",
    "\n",
    "# Standardize the features by removing the mean and scaling to unit variance. \n",
    "sc_X=StandardScaler()\n",
    "sc_X.fit(X_poly)  #Fit the scaller ONLY to the TRAINING DATA\n",
    "X_poly_normalized=sc_X.transform(X_poly) #Transform the training data \n",
    "\n",
    "#Check the range of values for the standardized features \n",
    "?\n",
    "\n",
    "#Add an extra column of 1' to X_poly\n",
    "X_poly_normalized = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map CV and test sets onto polynomial features. \n",
    "Apply the fitted (with the training data) scaler to them. \n",
    "\n",
    "DO NOT FIT AGAIN THE SCALER !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Xval and Xtest onto polynomial features and normalize\n",
    "X_poly_val =\n",
    "X_poly_test =\n",
    "\n",
    "# Apply the fitted StandardScaler \n",
    "#DO NOT FIT IT AGAIN (all scallers/normalizers are fitted only with the training data)\n",
    "X_poly_val_normalized =  ?\n",
    "X_poly_test_normalized=?\n",
    "\n",
    "#Add an extra column of 1' to each matrix\n",
    "X_poly_val_normalized = ?\n",
    "X_poly_test_normalized=?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Polynomial Regression\n",
    "\n",
    "The training of the polynomial model uses the same linear regression cost function and gradient as above. \n",
    "\n",
    "<img src=\"images/f7.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.5**: **Cost function trajectory** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n is now the new number of features after adding polynomial terms\n",
    "n= ?\n",
    "\n",
    "Lambda=0\n",
    "num_iter= ?\n",
    "learn_rate=0.001\n",
    "\n",
    "# inicialize theta vector with 0s\n",
    "init_theta= ?\n",
    "\n",
    "#call gradientDescent to get the optimal parameters theta\n",
    "theta_poly, J_history_poly = \n",
    "\n",
    "#Get Fig.5\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression - learning curve\n",
    "\n",
    "After training is over, you should get Figs. 6 and Fig.7 generated for $\\lambda$=0.\n",
    "The polynomial fit is able to follow the data points very well (Fig.6) and therefore the training error is very low (Fig.7). However, there is a gap between the training and CV errors (Fig.7), indicating a high variance problem due to the lack of regularization ( $\\lambda$=0). \n",
    "\n",
    "<img src=\"images/f5.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.6**: **Polynomial Fit** </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.7**: **Linear Regression learning curves** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fig.6\n",
    "#add the scatter plot of data (as above) \n",
    "?\n",
    "\n",
    "xmin = np.min(X) - 15\n",
    "xmax=np.max(X) + 25\n",
    "\n",
    "#Generate a grid of values for x coordinate\n",
    "x_value=np.linspace(xmin,xmax,2400)\n",
    "x_value=x_value.reshape(x_value.shape[0],1)\n",
    "\n",
    "# Map the x values and normalize\n",
    "x_value_poly = polyFeatures(x_value, degree)\n",
    "x_value_poly = sc_X.transform(x_value_poly)\n",
    "x_value_poly = np.append(np.ones((x_value_poly.shape[0],1)),x_value_poly, axis=1)\n",
    "\n",
    "#Compute the model prediction for x_value_poly\n",
    "y_value= np.dot(x_value_poly, theta_poly)\n",
    "\n",
    "plt.plot(x_value,y_value,\"--\",color=\"b\")\n",
    "plt.ylim(-5,160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call learningCurve to compute E_train and E_cv\n",
    "error_train, error_val = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fig.7\n",
    "\n",
    "plt.plot(range(m),error_train,label=\"Train\")\n",
    "plt.plot(range(m),error_val,label=\"Cross Validation\",color=\"r\")\n",
    "\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.ylim(0,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression with $\\lambda$ =  [0.001; 0.003; 0.01; 0.03; 0.1; 0.3; 1; 3; 10] ) \n",
    "\n",
    "Find the best $\\lambda$ from a grid search and compute the training, validation and test errors.\n",
    "(Consult function learningCurve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationCurve (X, y, Xval, yval, learn_rate, num_iter, Lambda_array):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the best lambda and the respective train and cross validation set errors\n",
    "    \"\"\"\n",
    "    m = ?  # Number of training examples \n",
    "    n=?  # number of features \n",
    "    mval = ?  # Number of validation examples \n",
    "    \n",
    "    error_train, error_val = [],[]\n",
    "    \n",
    "    for lam in Lambda_array:\n",
    "        # inicialize theta vector with 0s\n",
    "        theta_ini= ?\n",
    "        #call gradientDescent to compute the optimal theta \n",
    "        theta = ?\n",
    "                \n",
    "        #After the training is over, apply the trained model for train and validation data\n",
    "        pred_train = ?\n",
    "        pred_val =  ?\n",
    "        \n",
    "        #Compute the train and validation error\n",
    "        error_train_i = 1/(2*m) * np.sum((pred_train - y)**2)\n",
    "        error_val_i = 1/(2*mval) * np.sum((pred_val - yval)**2)\n",
    "        \n",
    "        error_train.append(error_train_i)\n",
    "        error_val.append(error_val_i)\n",
    "    \n",
    "    #Choose the best lambda to be the one that minimizes the validation error\n",
    "    ind = np.argmin(error_val)\n",
    "    best_lambda=Lambda_array[ind]\n",
    "        \n",
    "    return best_lambda, error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda_array = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "best_lambda, error_train, error_val = validationCurve(X_poly_normalized, y, X_poly_val_normalized, yval, learn_rate, num_iter, Lambda_array)\n",
    "\n",
    "print(best_lambda)\n",
    "\n",
    "plt.plot(Lambda_array,error_train,label=\"Train\")\n",
    "plt.plot(Lambda_array,error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "\n",
    "#for the best_lambda computed above, print the training (Etrain), validation (Eval) and test (Etest) errors\n",
    "\n",
    "# inicialize all theta at 0. \n",
    "theta_ini=?\n",
    "theta_poly = gradientDescent(X_poly_normalized, y,theta_ini,learn_rate,num_iter,best_lambda)[0]\n",
    "\n",
    "pred = np.dot(X_poly_normalized,theta_poly)\n",
    "Etrain = 1/(2*m) * np.sum((pred - y)**2)\n",
    "\n",
    "pred_val = np.dot(X_poly_val_normalized,theta_poly)\n",
    "Eval = 1/(2*mval) * np.sum((pred_val - yval)**2)\n",
    "\n",
    "pred_test = np.dot(X_poly_test_normalized, theta_poly)\n",
    "Etest = 1/(2*mtest) * np.sum((pred_test - ytest)**2)\n",
    "\n",
    "print(Etrain)\n",
    "print(Eval)\n",
    "print(Etest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
